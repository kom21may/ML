{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62c3afb2",
   "metadata": {},
   "source": [
    "## 1. A set of one-dimensional data points is given to you: 5, 10, 15, 20, 25, 30, 35. Assume that k = 2 and that the first set of random centroid is 15, 32, and that the second set is 12, 30.\n",
    "\n",
    "## a) Using the k-means method, create two clusters for each set of centroid described above.\n",
    "a) Creating clusters using the k-means method:\n",
    "\n",
    "1. Initial Centroids: 15, 32\n",
    "   - Iteration 1:\n",
    "     - Assign each data point to the nearest centroid:\n",
    "       - Cluster 1: 5, 10, 15\n",
    "       - Cluster 2: 20, 25, 30, 35\n",
    "     - Update the centroid positions:\n",
    "       - Cluster 1 centroid: (5 + 10 + 15) / 3 = 10\n",
    "       - Cluster 2 centroid: (20 + 25 + 30 + 35) / 4 = 27.5\n",
    "\n",
    "   - Iteration 2:\n",
    "     - Assign each data point to the nearest centroid:\n",
    "       - Cluster 1: 5, 10, 15\n",
    "       - Cluster 2: 20, 25, 30, 35\n",
    "     - Update the centroid positions:\n",
    "       - Cluster 1 centroid: (5 + 10 + 15) / 3 = 10\n",
    "       - Cluster 2 centroid: (20 + 25 + 30 + 35) / 4 = 27.5\n",
    "\n",
    "   - Iteration 3:\n",
    "     - Assign each data point to the nearest centroid:\n",
    "       - Cluster 1: 5, 10, 15\n",
    "       - Cluster 2: 20, 25, 30, 35\n",
    "     - Update the centroid positions:\n",
    "       - Cluster 1 centroid: (5 + 10 + 15) / 3 = 10\n",
    "       - Cluster 2 centroid: (20 + 25 + 30 + 35) / 4 = 27.5\n",
    "\n",
    "   The final clusters are:\n",
    "   - Cluster 1: 5, 10, 15\n",
    "   - Cluster 2: 20, 25, 30, 35\n",
    "\n",
    "2. Initial Centroids: 12, 30\n",
    "   - Iteration 1:\n",
    "     - Assign each data point to the nearest centroid:\n",
    "       - Cluster 1: 5, 10, 15\n",
    "       - Cluster 2: 20, 25, 30, 35\n",
    "     - Update the centroid positions:\n",
    "       - Cluster 1 centroid: (5 + 10 + 15) / 3 = 10\n",
    "       - Cluster 2 centroid: (20 + 25 + 30 + 35) / 4 = 27.5\n",
    "\n",
    "   - Iteration 2:\n",
    "     - Assign each data point to the nearest centroid:\n",
    "       - Cluster 1: 5, 10, 15\n",
    "       - Cluster 2: 20, 25, 30, 35\n",
    "     - Update the centroid positions:\n",
    "       - Cluster 1 centroid: (5 + 10 + 15) / 3 = 10\n",
    "       - Cluster 2 centroid: (20 + 25 + 30 + 35) / 4 = 27.5\n",
    "\n",
    "   - Iteration 3:\n",
    "     - Assign each data point to the nearest centroid:\n",
    "       - Cluster 1: 5, 10, 15\n",
    "       - Cluster 2: 20, 25, 30, 35\n",
    "     - Update the centroid positions:\n",
    "       - Cluster 1 centroid: (5 + 10 + 15) / 3 = 10\n",
    "       - Cluster 2 centroid: (20 + 25 + 30 + 35) / 4 = 27.5\n",
    "\n",
    "   The final clusters are:\n",
    "   - Cluster 1: 5, 10, 15\n",
    "   - Cluster 2: 20, 25, 30, 35\n",
    "\n",
    "\n",
    "## b) For each set of centroid values, calculate the SSE.\n",
    "\n",
    "b) Calculating SSE (Sum of Squared Errors) for each set of centroid values:\n",
    "\n",
    "1. Initial Centroids: 15, 32\n",
    "   - SSE = (Squared distance between each data point and its centroid within the cluster)\n",
    "   - Cluster 1: (5 - 15)^2 + (10 - 15)^2 + (15 - 15)^2 = 75\n",
    "   - Cluster 2: (20 - 32)^2 + (25 - 32)^2 + (30 - 32)^2 + (35 - 32)^2 = 100\n",
    "   - Total SSE = 75 + 100 = 175\n",
    "\n",
    "2. Initial Centroids: 12, 30\n",
    "   - SSE = (Squared distance between each data point and its centroid within the cluster)\n",
    "   - Cluster 1: (5 - 12)^2 + (10 - 12)^2 + (15 - 12)^2 = 14\n",
    "   - Cluster 2: (20 - 30)^2 + (25 - 30)^2 + (30 - 30)^2 + (35 - 30)^2 = 110\n",
    "   - Total SSE = 14 + 110 = 124\n",
    "\n",
    "Therefore, the SSE for the first set of centroids (15, 32) is 175, and the SSE for the second set of centroids (12, 30) is 124.\n",
    "\n",
    "## 2. Describe how the Market Basket Research makes use of association analysis concepts.\n",
    "How Market Basket Research uses association analysis concepts:\n",
    "\n",
    "- Association analysis is used in Market Basket Research to identify relationships and patterns between products based on customer purchase data. Here's how it makes use of association analysis concepts:\n",
    "\n",
    "1. Itemset Generation:\n",
    "   - Market Basket Research starts by generating itemsets, which are combinations of items that frequently co-occur in customer transactions.\n",
    "   - This step involves scanning the transaction data and counting the occurrence of each item or itemset.\n",
    "   - The Apriori algorithm is commonly used to efficiently generate frequent itemsets.\n",
    "\n",
    "2. Support Calculation:\n",
    "   - Support is a metric used to measure the frequency of occurrence of an itemset in the dataset.\n",
    "   - In Market Basket Research, support is calculated for each itemset and used to identify frequent itemsets that occur above a predefined support threshold.\n",
    "   - Frequent itemsets represent combinations of items that have a significant occurrence in customer transactions.\n",
    "\n",
    "3. Association Rule Generation:\n",
    "   - Once frequent itemsets are identified, association rules are generated.\n",
    "   - Association rules express relationships between itemsets, indicating that if certain items are present, other items are likely to be present as well.\n",
    "   - Association rules are of the form \"if X, then Y\" where X and Y are itemsets.\n",
    "\n",
    "4. Confidence and Lift Calculation:\n",
    "   - Confidence measures the strength of an association rule, representing the conditional probability that Y will be purchased given that X is purchased.\n",
    "   - Lift measures the significance of an association rule by comparing the observed support of the rule with the expected support if X and Y were independent.\n",
    "   - Association rules with high confidence and lift values indicate strong relationships between itemsets and are considered significant.\n",
    "\n",
    "5. Rule Evaluation and Selection:\n",
    "   - Generated association rules are evaluated based on metrics such as support, confidence, and lift.\n",
    "   - Rules that meet predefined thresholds for these metrics are considered significant and selected for further analysis.\n",
    "   - These rules provide insights into customer behavior, cross-selling opportunities, and can guide decision-making for marketing strategies.\n",
    "\n",
    "\n",
    "## 3. Give an example of the Apriori algorithm for learning association rules.\n",
    "Example of the Apriori algorithm for learning association rules:\n",
    "\n",
    "Consider the following example dataset of customer transactions:\n",
    "\n",
    "Transaction 1: Bread, Milk, Eggs\n",
    "Transaction 2: Bread, Diapers\n",
    "Transaction 3: Milk, Diapers, Beer\n",
    "Transaction 4: Bread, Milk, Diapers, Beer\n",
    "Transaction 5: Bread, Milk, Diapers, Beer\n",
    "\n",
    "Using the Apriori algorithm, we can find frequent itemsets and generate association rules:\n",
    "\n",
    "1. First Pass (Finding frequent items):\n",
    "   - Count the occurrence of each item: Bread (4), Milk (4), Diapers (4), Beer (3)\n",
    "   - Items with support above a minimum threshold (e.g., 3) are considered frequent: Bread, Milk, Diapers, Beer\n",
    "\n",
    "2. Second Pass (Finding frequent pairs):\n",
    "   - Generate all possible pairs from frequent items: {(Bread, Milk), (Bread, Diapers), (Bread, Beer), (Milk, Diapers), (Milk, Beer), (Diapers, Beer)}\n",
    "   - Count the occurrence of each pair in the transactions\n",
    "   - Pairs with support above the minimum threshold are considered frequent pairs\n",
    "\n",
    "3. Third Pass (Finding frequent triples):\n",
    "   - Generate all possible triples from frequent pairs and count their occurrence\n",
    "   - Frequent triples are identified based on the support threshold\n",
    "\n",
    "4. Association Rule Generation:\n",
    "   - Generate association rules from the frequent itemsets\n",
    "   - Calculate the confidence and lift for each rule\n",
    "   - Select rules that meet the confidence and lift thresholds\n",
    "\n",
    "For example, the generated association rule could be: {Milk, Diapers} -> {Beer} with a confidence of 0.75 and a lift of 1.5\n",
    "\n",
    "This rule suggests that if a customer buys Milk and Diapers, there is a 75% chance they will also purchase Beer, and the presence of Milk and Diapers together increases the likelihood of Beer being purchased by a factor of 1.5.\n",
    "## 4. In hierarchical clustering, how is the distance between clusters measured? Explain how this metric is used to decide when to end the iteration.\n",
    "- Distance measurement in hierarchical clustering and iteration termination:\n",
    "\n",
    "- In hierarchical clustering, the distance between clusters is measured using various distance metrics, such as Euclidean distance or Manhattan distance.\n",
    "- The choice of distance metric depends on the nature of the data and the problem at hand.\n",
    "- The distance between two clusters can be calculated based on the distance between their constituent data points or other methods like centroid distance or medoid distance.\n",
    "- The distance metric is used to determine the similarity or dissimilarity between clusters.\n",
    "\n",
    "Iteration termination in hierarchical clustering depends on the specific algorithm used:\n",
    "\n",
    "- Agglomerative Hierarchical Clustering: Iterations continue until all data points are in a single cluster or until a predefined number of clusters is reached.\n",
    "- Divisive Hierarchical Clustering: Iterations continue until each data point is in its own individual cluster or until a predefined number of clusters is reached.\n",
    "\n",
    "The decision to end the iteration is typically based on criteria such as a specific number of clusters desired, a threshold distance value, or a predefined stopping criterion.\n",
    "## 5. In the k-means algorithm, how do you recompute the cluster centroids?\n",
    "- Recomputing cluster centroids in the k-means algorithm:\n",
    "\n",
    "- In the k-means algorithm, cluster centroids are recomputed in each iteration to update the cluster assignments.\n",
    "- The process of recomputing cluster centroids involves the following steps:\n",
    "\n",
    "1. Assign data points to the nearest centroid: Calculate the distance between each data point and each centroid and assign the data point to the nearest centroid.\n",
    "\n",
    "2. Update the centroid position: Once the data points are assigned to the centroids, update the centroid positions by calculating the mean of the data points within each cluster.\n",
    "   - Compute the mean position of the data points in each cluster along each dimension to obtain the new centroid position.\n",
    "\n",
    "3. Repeat the process: Repeat steps 1 and 2 until convergence, where the cluster assignments and centroid positions stabilize.\n",
    "\n",
    "By iteratively recomputing the cluster centroids, the k-means algorithm aims to minimize the sum of squared distances between data points and their assigned centroids, resulting in compact and well-separated clusters.\n",
    "## 6. At the start of the clustering exercise, discuss one method for determining the required number of clusters.\n",
    "- Determining the required number of clusters in the clustering exercise:\n",
    "\n",
    "- One method for determining the required number of clusters at the start of the clustering exercise is the Elbow Method.\n",
    "- The Elbow Method involves the following steps:\n",
    "\n",
    "1. Run the k-means algorithm for a range of potential cluster numbers (e.g., from 1 to 10).\n",
    "\n",
    "2. For each cluster number, calculate the sum of squared distances (SSE) between data points and their assigned centroids.\n",
    "\n",
    "3. Plot the SSE values against the corresponding cluster numbers.\n",
    "\n",
    "4. Look for the \"elbow\" point on the plot, which is the point where the SSE starts to level off or decrease at a slower rate.\n",
    "\n",
    "5. The number of clusters corresponding to the elbow point is considered a reasonable choice for the required number of clusters.\n",
    "\n",
    "The idea behind the Elbow Method is to find the number of clusters that provides a good balance between minimizing the SSE and avoiding excessive fragmentation or overfitting.\n",
    "\n",
    "## 7. Discuss the k-means algorithm&#39;s advantages and disadvantages.\n",
    "\n",
    "Advantages and disadvantages of the k-means algorithm:\n",
    "\n",
    "Advantages:\n",
    "- Simple and easy to implement.\n",
    "- Efficient for large datasets.\n",
    "- Works well when clusters are spherical and well-separated.\n",
    "- Provides fast convergence.\n",
    "\n",
    "Disadvantages:\n",
    "- Requires the number of clusters to be specified in advance.\n",
    "- Sensitive to the initial selection of centroids, which can lead to different outcomes.\n",
    "- Prone to the influence of outliers.\n",
    "- Cannot handle non-linear data or clusters with irregular shapes.\n",
    "\n",
    "\n",
    "\n",
    "## 8. Draw a diagram to demonstrate the principle of clustering.\n",
    " - Diagram illustrating the principle of clustering:\n",
    "\n",
    "[Diagram demonstrating clustering]\n",
    "\n",
    "    A diagram illustrating the principle of clustering typically shows a scatter plot of data points with different colors or symbols representing different clusters. The plot demonstrates the process of grouping similar data points together to form distinct clusters. The clusters are shown as tightly packed groups with minimal overlap between them. The diagram visually highlights the separation and cohesion of the clusters, indicating the effectiveness of the clustering algorithm in identifying meaningful patterns or groups within the data.\n",
    "## 9. During your study, you discovered seven findings, which are listed in the data points below. Using the K-means algorithm, you want to build three clusters from these observations. \n",
    "##### The clusters C1, C2, and C3 have the following findings after the first iteration:\n",
    "\n",
    "##### C1: (2,2), (4,4), (6,6); C2: (2,2), (4,4), (6,6); C3: (2,2), (4,4),\n",
    "\n",
    "##### C2: (0,4), (4,0), (0,4), (0,4), (0,4), (0,4), (0,4), (0,4), (0,\n",
    "\n",
    "##### C3: (5,5) and (9,9)\n",
    "\n",
    "##### What would the cluster centroids be if you were to run a second iteration? What would this clustering's SSE be?\n",
    "\n",
    "To determine the cluster centroids and SSE for the second iteration, we need to compute the mean position of the data points in each cluster. \n",
    "\n",
    "Given the following clusters after the first iteration:\n",
    "\n",
    "C1: (2,2), (4,4), (6,6)\n",
    "C2: (2,2), (4,4), (6,6)\n",
    "C3: (2,2), (4,4), (5,5), (9,9)\n",
    "\n",
    "Calculating the mean position for each cluster:\n",
    "\n",
    "C1: (2+4+6)/3, (2+4+6)/3 = (4, 4)\n",
    "C2: (2+4+6)/3, (2+4+6)/3 = (4, 4)\n",
    "C3: (2+4+5+9)/4, (2+4+5+9)/4 = (5, 5)\n",
    "\n",
    "The cluster centroids after the second iteration are:\n",
    "\n",
    "C1: (4, 4)\n",
    "C2: (4, 4)\n",
    "C3: (5, 5)\n",
    "\n",
    "To calculate the SSE (Sum of Squared Errors), we need to compute the squared distance between each data point and its assigned centroid, and then sum up these squared distances for all data points.\n",
    "\n",
    "SSE for the second iteration:\n",
    "\n",
    "- For C1: SSE(C1) = [(2-4)^2 + (2-4)^2] + [(4-4)^2 + (4-4)^2] + [(6-4)^2 + (6-4)^2] = 8 + 0 + 8 = 16\n",
    "- For C2: SSE(C2) = [(2-4)^2 + (2-4)^2] + [(4-4)^2 + (4-4)^2] + [(6-4)^2 + (6-4)^2] = 8 + 0 + 8 = 16\n",
    "- For C3: SSE(C3) = [(2-5)^2 + (2-5)^2] + [(4-5)^2 + (4-5)^2] + [(5-5)^2 + (5-5)^2] + [(9-5)^2 + (9-5)^2] = 18 + 2 + 0 + 32 = 52\n",
    "\n",
    "Total SSE for the second iteration: SSE = SSE(C1) + SSE(C2) + SSE(C3) = 16 + 16 + 52 = 84\n",
    "\n",
    "Therefore, the cluster centroids after the second iteration would be C1: (4, 4), C2: (4, 4), and C3: (5, 5). The SSE for this clustering would be 84.\n",
    "\n",
    "##  10. In a software project, the team is attempting to determine if software flaws discovered during testing are identical. Based on the text analytics of the defect details, they decided to build 5 clusters of related defects. Any new defect formed after the 5 clusters of defects have been identified must be listed as one of the forms identified by clustering. A simple diagram can be used to explain this process. Assume you have 20 defect data points that are clustered into 5 clusters and you used the k-means algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f68e39b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
